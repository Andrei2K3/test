# main_pipeline
## Isolation Forest
Am incarcat si preprocesat dataset-urile folosind doua variante de preprocessare, reducand coloanele la cele cu adevarat utile(erau cateva coloane goale). Am rulat Isolation Forest pe setul normal si am identificat 5 anomalii, apoi am testat modelul pe MEDIUM, HIGH si RAMP-UP observand diferente mari intre scoruri si numarul de anomalii. Am calculat importanta nesupravegheata a feature-urilor si am descoperit ca anumite coloane sunt complet constante sau goale, deci am mai redus inca o data numarul de coloane. Am aplicat, de asemenea, un threshold pe scoruri pentru a compara IF cu o abordare pur statistica, care detecteaza mult mai multe anomalii. Am combinat NORMAL+MEDIUM+HIGH intr-un model unificat Isolation Forest(prima data am antrenat doar pe dataset-ul NORMAL, acum am antrenat pe toate cele 3 dataset-uri), care detecteaza anomalii mai stabil, inclusiv pe scenariul RAMP-UP. Am generat statistici detaliate (raport anomalii, distributia scorurilor, burst-uri). 

## Dbscan
Am rulat DBSCAN pentru detectare alternativa, observand o rata mare de outlieri. Am facut grid search pe eps si min_samples. 

## Concluzie
Isolation Forest e stabil daca antrenarea se face pe toate nivelele (toate cele 3 dataset-uri), iar DBSCAN produce prea multe anomalii fara a incercat sa-i facem un tuning.

# experiment_2

Am incarcat datele pentru cele patru scenarii (Normal, Medium, High si Ramp) si le-am curatat pastrand doar coloanele relevante, plus cateva statistici temporale. Dupa scalare, am rulat patru metode de detectie a anomaliilor: Isolation Forest, Elliptic Envelope, Z-score si o combinatie (Ensemble) care tine cont de toate. Primele doua metode au detectat aproximativ 5 anomalii(asa am impus eu, sa-mi detecteze doar 5 prin intermediul parametrului contamination), in timp ce Z-score a marcat mult prea multe puncte ca fiind suspecte. Ensemble-ul a oferit cele mai echilibrate rezultate, gasind doar anomaliile care apar in mod consecvent. Am analizat valorile pentru RSSI, SNR si throughput in zonele anormale si am vizualizat rezultatele in timeline si PCA pentru a vedea gruparile naturale. In comparatia finala, procentul de anomalii creste odata cu incarcarea traficului: Normal (2.05), Medium (2.88), High (6.93) si Ramp (3.43), ceea ce confirma comportamentul asteptat al retelei.
Mentiune: am aflat ulterior, ca Elliptic si Z-Score nu sunt deloc bune pentru dataset-uri de telecom, unde datele nu sunt distribuite uniform..

# experiment_3
Am incarcat cele patru seturi de date (Normal, Medium, High, Ramp) si am generat *un alt set de features* (am zis sa incerc, sa-mi creez alte features in speranta ca pot sa obtin rezultate mai bune..) : eficienta a semnalului, volatilitate, rate of change si statistici rolling. Modelul a fost antrenat doar pe datele normale, apoi am creat seturi mixte (Normal + Medium/High) pentru validare si testare, astfel incat scorurile F1 si ROC-AUC sa fie realiste. Am testat mai multe niveluri de contaminare si am descoperit ca un prag de 0.15 ofera cel mai bun F1-score pe Medium Load. Evaluarea finala a aratat performante excelente: 0.97 F1 si ROC-AUC peste 0.98 atat pe Medium, cat si pe High. Vizualizarile timeline si PCA confirma ca modelul separa foarte bine observatiile anormale. La final, am salvat modelul, scaler-ul si configuratia pentru reproducere si deploy. Pipeline-ul final este stabil, optimizat si adaptat pentru detectarea anomaliilor in conditii de incarcare variabila a retelei.
Mentiune: Modelul este antrenat pe Normal si din ce-am citit pe internet poate fi considerata anomalie si cand un dispozitiv trece pe Medium sau High, deoarece inseamna o abatere de la comportamentul de baza. Din aceasta cauza pe grafice sunt foarte multe anomalii.  
